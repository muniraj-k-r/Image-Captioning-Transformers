# ğŸ–¼ï¸ Image Captioning Using Transformers and Attention Mechanism

This project demonstrates a deep learning-based image captioning system that generates natural language descriptions for input images. It leverages **Convolutional Neural Networks (CNNs)** for image feature extraction and a **Transformer-based decoder with Attention Mechanism** for caption generation.

---

## ğŸ“Œ Table of Contents
- [Overview](#overview)
- [Tech Stack](#tech-stack)
- [Architecture](#architecture)
- [How It Works](#how-it-works)
- [Installation](#installation)
- [How to Run](#how-to-run)
- [Results](#results)
- [Use Cases](#use-cases)
- [Limitations](#limitations)
- [Future Improvements](#future-improvements)
- [Author](#author)

---

## âœ… Overview

> "Automatically describe images in natural language using a combination of computer vision and natural language processing."

- Extracts features using a pre-trained CNN (e.g., InceptionV3)
- Uses Transformer layers with self-attention to generate accurate captions
- Trained and tested on the **Flickr8k** dataset
- Built with **Python, TensorFlow, and Keras**

---

## ğŸ§° Tech Stack

- **Language**: Python 3.x  
- **Deep Learning**: TensorFlow / Keras  
- **Modeling**: CNN (for feature extraction), Transformer (for language modeling)  
- **Dataset**: Flickr8k  
- **Jupyter Notebook**: for experimentation and visualization  
- **Matplotlib / PIL**: for displaying results  

---

## ğŸ” How It Works

1. **Image Encoding**: A CNN (e.g., InceptionV3) extracts image features as a dense vector.
2. **Attention Mechanism**: Focuses on specific parts of the image relevant to each word.
3. **Transformer Decoder**: Takes image features + previously generated words to predict the next word.
4. **Output**: A complete sentence describing the image.

---

## ğŸ› ï¸ Installation

1. Clone the repository:

- git clone https://github.com/muniraj-k-r/Image-Captioning-Transformers.git

2. cd Image-Captioning-Transformers

---
---
## 2.Install required libraries:

- pip install -r requirements.txt

--- 
## â–¶ï¸ How to Run
- python main.py
- Or if using Jupyter:

- jupyter notebook ImageCaptioningTransformer.ipynb
- Make sure the Flickr8k dataset and pre-trained model are available or downloaded as instructed in the code.

---
## ğŸ“Š Results
Here are some examples of model-generated captions:

| Image         | Generated Caption                    |
| ------------- | ------------------------------------ |
| ğŸ¶ `dog.jpg`  | *"a dog running in the grass"*       |
| ğŸ§’ `kids.jpg` | *"two children are playing outside"* |
---
---
## ğŸ’¡ Use Cases
1.Assistive technology for the visually impaired

2.Automatic image tagging on social media

3.Enhanced image search engines

4.Real-time captioning in surveillance systems

---
## âš ï¸ Limitations
May generate generic or grammatically incorrect captions

Performance depends on the dataset (e.g., Flickr8k is small)

Requires GPU for faster training and inference

---
## ğŸš€ Future Improvements
Use larger datasets like MS-COCO for better accuracy

Deploy using Streamlit or FastAPI as a web app

Add beam search for improved caption generation

Visualize attention maps for each word

---
## ğŸ‘¨â€ğŸ’» Author
Muniraj K R
- ğŸ“ BNM Institute of Technology
- ğŸŒ GitHub: muniraj-k-r
